---
tags:
  - formation
  - cloud
  - architecture
  - haute-disponibilite
  - disaster-recovery
  - scalabilite
---

# Module 5 : Architecture Cloud (HA, DR, ScalabilitÃ©)

## Objectifs du Module

Ã€ la fin de ce module, vous serez capable de :

- :fontawesome-solid-heart-pulse: Concevoir une architecture haute disponibilitÃ©
- :fontawesome-solid-rotate: Planifier une stratÃ©gie de Disaster Recovery
- :fontawesome-solid-arrows-up-down: Comprendre les types de scalabilitÃ©
- :fontawesome-solid-diagram-project: ReconnaÃ®tre les patterns d'architecture cloud
- :fontawesome-solid-calculator: Calculer et interprÃ©ter les SLA

---

## 1. Haute DisponibilitÃ© (HA)

### 1.1 Qu'est-ce que la Haute DisponibilitÃ© ?

!!! info "DÃ©finition"
    La **haute disponibilitÃ©** est la capacitÃ© d'un systÃ¨me Ã  rester opÃ©rationnel malgrÃ© la dÃ©faillance de certains composants.

```mermaid
graph TB
    subgraph "âŒ Sans HA"
        SINGLE["ğŸ’» Serveur unique"]
        SINGLE --> FAIL["ğŸ’¥ Panne = Service DOWN"]
    end

    subgraph "âœ… Avec HA"
        LB["âš–ï¸ Load Balancer"]
        S1["ğŸ’» Serveur 1"]
        S2["ğŸ’» Serveur 2"]
        S3["ğŸ’» Serveur 3"]
        LB --> S1
        LB --> S2
        LB --> S3
        S1 --> OK["Panne S1 = Service OK"]
    end

    style FAIL fill:#f44336,color:#fff
    style OK fill:#4caf50,color:#fff
```

### 1.2 Les Niveaux de DisponibilitÃ© (SLA)

| DisponibilitÃ© | Downtime/an | Downtime/mois | CatÃ©gorie |
|---------------|-------------|---------------|-----------|
| **99%** | 3.65 jours | 7.3 heures | Standard |
| **99.9%** | 8.76 heures | 43.8 minutes | Ã‰levÃ© |
| **99.95%** | 4.38 heures | 21.9 minutes | TrÃ¨s Ã©levÃ© |
| **99.99%** | 52.6 minutes | 4.38 minutes | Critique |
| **99.999%** | 5.26 minutes | 26.3 secondes | Mission critique |

!!! example "SLA Cloud Providers"
    | Service | AWS | Azure | GCP |
    |---------|-----|-------|-----|
    | **VM unique** | 99.5% | 99.9% | 99.5% |
    | **VM multi-AZ** | 99.99% | 99.99% | 99.99% |
    | **Object Storage** | 99.99% | 99.99% | 99.99% |
    | **Managed DB multi-AZ** | 99.95% | 99.99% | 99.95% |

### 1.3 Calcul du SLA Composite

```mermaid
graph LR
    LB["Load Balancer<br/>99.99%"] --> APP["App (2 VMs)<br/>99.99%"]
    APP --> DB["Database<br/>99.95%"]

    TOTAL["SLA Total = ?"]

    style TOTAL fill:#ff9800,color:#fff
```

**Formule** : SLA composite = SLA1 Ã— SLA2 Ã— SLA3

```
SLA = 99.99% Ã— 99.99% Ã— 99.95% = 99.93%
```

!!! warning "Attention"
    Chaque composant **diminue** la disponibilitÃ© globale. Plus vous avez de composants, plus le SLA baisse.

### 1.4 Patterns de Haute DisponibilitÃ©

#### Active-Active

```mermaid
graph TB
    LB["âš–ï¸ Load Balancer"]
    subgraph "Zone A"
        A1["ğŸ’» Server A1<br/>ACTIVE"]
    end
    subgraph "Zone B"
        A2["ğŸ’» Server A2<br/>ACTIVE"]
    end

    LB --> A1
    LB --> A2

    style A1 fill:#4caf50,color:#fff
    style A2 fill:#4caf50,color:#fff
```

- Les deux serveurs traitent le trafic
- CapacitÃ© = Server1 + Server2
- Si un tombe, l'autre absorbe la charge

#### Active-Passive (Standby)

```mermaid
graph TB
    LB["âš–ï¸ Load Balancer"]
    subgraph "Zone A"
        A1["ğŸ’» Server A1<br/>ACTIVE"]
    end
    subgraph "Zone B"
        A2["ğŸ’» Server A2<br/>STANDBY"]
    end

    LB --> A1
    A1 -.->|"Failover"| A2

    style A1 fill:#4caf50,color:#fff
    style A2 fill:#ff9800,color:#fff
```

- Un seul serveur traite le trafic
- Le second attend en standby
- Bascule automatique en cas de panne

---

## 2. Disaster Recovery (DR)

### 2.1 RPO et RTO

```mermaid
graph LR
    subgraph "Timeline d'un Incident"
        BACKUP["ğŸ“¸ Dernier Backup"]
        DISASTER["ğŸ’¥ Incident"]
        RECOVERY["âœ… Reprise"]
    end

    BACKUP -->|"RPO<br/>(donnÃ©es perdues)"| DISASTER
    DISASTER -->|"RTO<br/>(temps d'arrÃªt)"| RECOVERY

    style DISASTER fill:#f44336,color:#fff
    style RECOVERY fill:#4caf50,color:#fff
```

| MÃ©trique | DÃ©finition | Question |
|----------|------------|----------|
| **RPO** (Recovery Point Objective) | QuantitÃ© de donnÃ©es qu'on accepte de perdre | "Combien de temps de donnÃ©es perdues est acceptable ?" |
| **RTO** (Recovery Time Objective) | Temps pour rÃ©tablir le service | "En combien de temps doit-on Ãªtre de nouveau opÃ©rationnel ?" |

### 2.2 StratÃ©gies de DR

```mermaid
graph TB
    subgraph "StratÃ©gies DR (coÃ»t croissant)"
        BACKUP["ğŸ’¾ Backup/Restore<br/>RTO: heures/jours<br/>CoÃ»t: $"]
        PILOT["ğŸ”¥ Pilot Light<br/>RTO: dizaines de minutes<br/>CoÃ»t: $$"]
        WARM["â™¨ï¸ Warm Standby<br/>RTO: minutes<br/>CoÃ»t: $$$"]
        HOT["ğŸ”¥ Hot Standby<br/>RTO: secondes<br/>CoÃ»t: $$$$"]
    end

    BACKUP --> PILOT --> WARM --> HOT

    style BACKUP fill:#4caf50,color:#fff
    style PILOT fill:#8bc34a,color:#fff
    style WARM fill:#ff9800,color:#fff
    style HOT fill:#f44336,color:#fff
```

| StratÃ©gie | Description | RTO | RPO | CoÃ»t |
|-----------|-------------|-----|-----|------|
| **Backup/Restore** | DonnÃ©es sauvegardÃ©es, infra recrÃ©Ã©e | Heures/Jours | Heures | $ |
| **Pilot Light** | Core infra tourne, reste Ã  dÃ©marrer | 10-30 min | Minutes | $$ |
| **Warm Standby** | Infra complÃ¨te tourne Ã  Ã©chelle rÃ©duite | Minutes | Minutes | $$$ |
| **Hot Standby** | Infra identique en active-active | Secondes | Temps rÃ©el | $$$$ |

### 2.3 Architecture Multi-RÃ©gion

```mermaid
graph TB
    subgraph "Region: Europe (Primary)"
        LB1["âš–ï¸ LB"]
        APP1["ğŸ’» App"]
        DB1["ğŸ—„ï¸ DB Primary"]
    end

    subgraph "Region: US (DR)"
        LB2["âš–ï¸ LB"]
        APP2["ğŸ’» App (scaled down)"]
        DB2["ğŸ—„ï¸ DB Replica"]
    end

    DNS["ğŸŒ DNS (Route 53, Traffic Manager)"]

    USER["ğŸ‘¥ Users"] --> DNS
    DNS -->|"Normal"| LB1
    DNS -.->|"Failover"| LB2
    DB1 -->|"Replication"| DB2

    style DB1 fill:#4caf50,color:#fff
    style DB2 fill:#ff9800,color:#fff
```

---

## 3. ScalabilitÃ©

### 3.1 Scaling Vertical vs Horizontal

```mermaid
graph TB
    subgraph "â¬†ï¸ Scaling Vertical (Scale Up)"
        V1["ğŸ’» Small<br/>2 CPU, 4GB"]
        V2["ğŸ’» Medium<br/>4 CPU, 8GB"]
        V3["ğŸ’» Large<br/>8 CPU, 16GB"]
        V1 --> V2 --> V3
    end

    subgraph "â¡ï¸ Scaling Horizontal (Scale Out)"
        H1["ğŸ’» Server 1"]
        H2["ğŸ’» Server 2"]
        H3["ğŸ’» Server 3"]
        H4["ğŸ’» Server 4"]
    end

    style V3 fill:#4caf50,color:#fff
    style H4 fill:#2196f3,color:#fff
```

| Type | Description | Avantages | InconvÃ©nients |
|------|-------------|-----------|---------------|
| **Vertical** | Augmenter la taille d'une machine | Simple, pas de changement d'archi | Limite physique, downtime |
| **Horizontal** | Ajouter plus de machines | Pas de limite, HA naturelle | ComplexitÃ© (stateless, LB) |

### 3.2 Auto Scaling

```mermaid
graph LR
    subgraph "Auto Scaling"
        METRIC["ğŸ“Š MÃ©triques<br/>(CPU, RAM, Requests)"]
        POLICY["ğŸ“œ Politique<br/>Si CPU > 70%"]
        ACTION["âš¡ Action<br/>Ajouter 2 instances"]
    end

    METRIC --> POLICY --> ACTION

    style POLICY fill:#ff9800,color:#fff
```

**MÃ©triques courantes pour le scaling :**

| MÃ©trique | Description | Exemple de seuil |
|----------|-------------|------------------|
| **CPU** | Utilisation processeur | > 70% â†’ scale up |
| **Memory** | Utilisation mÃ©moire | > 80% â†’ scale up |
| **Requests/sec** | Nombre de requÃªtes | > 1000 req/s â†’ scale up |
| **Queue depth** | Messages en attente | > 100 messages â†’ scale up |
| **Custom** | MÃ©trique business | > X transactions/min |

### 3.3 Stateless vs Stateful

!!! warning "ClÃ© du Scaling Horizontal"
    Pour scaler horizontalement, votre application doit Ãªtre **stateless** (sans Ã©tat local).

```mermaid
graph TB
    subgraph "âŒ Stateful (Non scalable)"
        S1["ğŸ’» Server<br/>Session en mÃ©moire"]
        USER1["ğŸ‘¤ User A<br/>Session sur Server 1"]
    end

    subgraph "âœ… Stateless (Scalable)"
        LB["âš–ï¸ Load Balancer"]
        SS1["ğŸ’» Server 1"]
        SS2["ğŸ’» Server 2"]
        SS3["ğŸ’» Server 3"]
        CACHE["âš¡ Redis<br/>(Sessions)"]

        LB --> SS1
        LB --> SS2
        LB --> SS3
        SS1 --> CACHE
        SS2 --> CACHE
        SS3 --> CACHE
    end

    style S1 fill:#f44336,color:#fff
    style CACHE fill:#4caf50,color:#fff
```

**Comment rendre une app stateless :**

- Sessions â†’ Store externe (Redis, DynamoDB)
- Fichiers uploadÃ©s â†’ Object Storage (S3)
- Cache â†’ Cache distribuÃ© (ElastiCache)
- Base de donnÃ©es â†’ Service managÃ© (RDS)

---

## 4. Patterns d'Architecture Cloud

### 4.1 Architecture N-Tier

```mermaid
graph TB
    subgraph "Presentation Tier"
        WEB["ğŸŒ Web Servers<br/>(Frontend)"]
    end

    subgraph "Application Tier"
        APP["âš™ï¸ App Servers<br/>(Backend API)"]
    end

    subgraph "Data Tier"
        DB["ğŸ—„ï¸ Database"]
        CACHE["âš¡ Cache"]
    end

    WEB --> APP
    APP --> DB
    APP --> CACHE

    style WEB fill:#2196f3,color:#fff
    style APP fill:#4caf50,color:#fff
    style DB fill:#ff9800,color:#fff
```

### 4.2 Microservices

```mermaid
graph TB
    subgraph "Microservices Architecture"
        GW["ğŸšª API Gateway"]

        subgraph "Services"
            SVC1["ğŸ‘¤ User Service"]
            SVC2["ğŸ’³ Payment Service"]
            SVC3["ğŸ“¦ Order Service"]
            SVC4["ğŸ“§ Notification Service"]
        end

        subgraph "Data"
            DB1["ğŸ—„ï¸ User DB"]
            DB2["ğŸ—„ï¸ Payment DB"]
            DB3["ğŸ—„ï¸ Order DB"]
        end

        QUEUE["ğŸ“¬ Message Queue"]
    end

    GW --> SVC1
    GW --> SVC2
    GW --> SVC3
    SVC1 --> DB1
    SVC2 --> DB2
    SVC3 --> DB3
    SVC3 --> QUEUE
    QUEUE --> SVC4

    style GW fill:#9c27b0,color:#fff
```

**Avantages :**
- Scaling indÃ©pendant par service
- DÃ©ploiement indÃ©pendant
- Technologie adaptÃ©e par service
- Ã‰quipes autonomes

### 4.3 Event-Driven Architecture

```mermaid
graph LR
    subgraph "Producers"
        P1["ğŸ“± Mobile App"]
        P2["ğŸŒ Web App"]
        P3["âš™ï¸ Backend"]
    end

    BROKER["ğŸ“¬ Event Broker<br/>(Kafka, EventBridge)"]

    subgraph "Consumers"
        C1["ğŸ“Š Analytics"]
        C2["ğŸ“§ Notifications"]
        C3["ğŸ—„ï¸ Data Lake"]
    end

    P1 --> BROKER
    P2 --> BROKER
    P3 --> BROKER
    BROKER --> C1
    BROKER --> C2
    BROKER --> C3

    style BROKER fill:#ff9800,color:#fff
```

---

## 5. Well-Architected Framework

### 5.1 Les 6 Piliers

```mermaid
mindmap
  root((Well-Architected))
    Operational Excellence
      Automatisation
      ObservabilitÃ©
      AmÃ©lioration continue
    Security
      IAM
      DÃ©tection
      Protection donnÃ©es
    Reliability
      HA
      DR
      Gestion pannes
    Performance Efficiency
      SÃ©lection ressources
      Monitoring
      Optimisation
    Cost Optimization
      FinOps
      Right-sizing
      Reserved capacity
    Sustainability
      EfficacitÃ© Ã©nergÃ©tique
      Impact environnemental
```

### 5.2 Questions ClÃ©s par Pilier

| Pilier | Questions Ã  se poser |
|--------|----------------------|
| **Operational Excellence** | Comment dÃ©ployez-vous ? Comment dÃ©tectez-vous les problÃ¨mes ? |
| **Security** | Qui a accÃ¨s ? Comment protÃ©gez-vous les donnÃ©es ? |
| **Reliability** | Que se passe-t-il si X tombe ? Quel est votre RTO/RPO ? |
| **Performance** | Comment gÃ©rez-vous les pics ? Avez-vous des goulots ? |
| **Cost Optimization** | Payez-vous pour des ressources inutilisÃ©es ? |
| **Sustainability** | Quel est l'impact carbone ? Pouvez-vous optimiser ? |

---

## 6. Quiz de Validation

!!! question "Question 1"
    Quelle est la diffÃ©rence entre RPO et RTO ?

    ??? success "RÃ©ponse"
        - **RPO** (Recovery Point Objective) : QuantitÃ© de donnÃ©es acceptable Ã  perdre (temps depuis le dernier backup)
        - **RTO** (Recovery Time Objective) : Temps acceptable pour restaurer le service

        Exemple : RPO de 1h signifie qu'on accepte de perdre 1h de donnÃ©es. RTO de 30min signifie qu'on doit Ãªtre opÃ©rationnel en 30 minutes.

!!! question "Question 2"
    Si vous avez 3 composants avec des SLA de 99.9%, 99.95% et 99.9%, quel est le SLA composite ?

    ??? success "RÃ©ponse"
        SLA = 99.9% Ã— 99.95% Ã— 99.9% = **99.75%**

        Le SLA composite est toujours infÃ©rieur au SLA le plus faible.

!!! question "Question 3"
    Quelle stratÃ©gie DR a le RTO le plus court ?

    ??? success "RÃ©ponse"
        **Hot Standby** (Active-Active)

        L'infrastructure de DR est identique et tourne en permanence. Le failover est quasi-instantanÃ© (secondes).

!!! question "Question 4"
    Pourquoi une application doit-elle Ãªtre stateless pour scaler horizontalement ?

    ??? success "RÃ©ponse"
        Si l'Ã©tat (session, fichiers) est stockÃ© localement sur un serveur, les requÃªtes suivantes doivent aller sur le mÃªme serveur. Impossible de distribuer la charge.

        Avec une app stateless, n'importe quel serveur peut traiter n'importe quelle requÃªte.

---

## 7. RÃ©sumÃ©

| Concept | Description | MÃ©trique clÃ© |
|---------|-------------|--------------|
| **Haute DisponibilitÃ©** | Service reste up malgrÃ© pannes | SLA (99.9%, 99.99%...) |
| **Disaster Recovery** | Reprise aprÃ¨s sinistre majeur | RTO, RPO |
| **ScalabilitÃ© Verticale** | Augmenter la taille | Limite physique |
| **ScalabilitÃ© Horizontale** | Ajouter des instances | Stateless requis |
| **Auto Scaling** | Scaling automatique | MÃ©triques, seuils |

---

## Exercice : Ã€ Vous de Jouer

!!! example "Mise en Pratique"
    **Objectif** : Concevoir une architecture haute disponibilitÃ© avec stratÃ©gie DR

    **Contexte** : Une application critique de paiement en ligne nÃ©cessite un SLA de 99.99% et des objectifs RPO=15min / RTO=1h. L'application traite 10 000 transactions/jour en temps normal, mais jusqu'Ã  100 000 lors des soldes.

    **TÃ¢ches Ã  rÃ©aliser** :

    1. Calculez le SLA composite de l'architecture : ALB (99.99%) + App servers (99.99%) + RDS (99.95%)
    2. Proposez une architecture multi-AZ pour garantir la haute disponibilitÃ©
    3. DÃ©finissez la stratÃ©gie DR adaptÃ©e (Backup/Pilot Light/Warm/Hot) pour respecter RPO/RTO
    4. Configurez l'auto-scaling pour gÃ©rer les pics de charge (x10)

    **CritÃ¨res de validation** :

    - [ ] SLA composite calculÃ© correctement
    - [ ] Architecture multi-AZ avec failover automatique
    - [ ] StratÃ©gie DR justifiÃ©e avec RPO/RTO respectÃ©s
    - [ ] Configuration auto-scaling adaptÃ©e aux pics

??? quote "Solution"
    **1. Calcul du SLA composite**

    ```
    SLA composite = SLA1 Ã— SLA2 Ã— SLA3
    SLA = 99.99% Ã— 99.99% Ã— 99.95%
    SLA = 0.9999 Ã— 0.9999 Ã— 0.9995
    SLA = 0.9993 = 99.93%

    Downtime annuel = (1 - 0.9993) Ã— 365 Ã— 24 Ã— 60
    Downtime = 6.13 heures/an â‰ˆ 30 minutes/mois
    ```

    **âš ï¸ Le SLA de 99.93% ne respecte pas l'objectif 99.99%**
    â†’ Solution : Dupliquer les composants critiques

    **2. Architecture multi-AZ haute disponibilitÃ©**

    ```bash
    # Auto Scaling Group multi-AZ
    aws autoscaling create-auto-scaling-group \
      --auto-scaling-group-name payment-api-asg \
      --launch-template payment-api-template \
      --min-size 4 \
      --max-size 40 \
      --desired-capacity 6 \
      --availability-zones eu-west-3a eu-west-3b eu-west-3c \
      --target-group-arns arn:aws:elasticloadbalancing:xxx

    # RDS Multi-AZ avec read replicas
    aws rds create-db-instance \
      --db-instance-identifier payment-db \
      --multi-az \
      --backup-retention-period 7
    ```

    **Architecture :**
    ```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           Region: eu-west-3 (Paris)             â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   Zone A    â”‚   Zone B    â”‚       Zone C        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  App x2     â”‚  App x2     â”‚      App x2         â”‚
    â”‚  RDS        â”‚  RDS        â”‚                     â”‚
    â”‚  Primary    â”‚  Standby    â”‚   Read Replica      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```

    **3. StratÃ©gie DR : Warm Standby**

    **Justification :**
    - RPO 15min â†’ RÃ©plication continue nÃ©cessaire
    - RTO 1h â†’ Infrastructure prÃ©-dÃ©ployÃ©e mais rÃ©duite
    - Warm Standby = meilleur compromis coÃ»t/performance

    ```bash
    # RÃ©gion DR (eu-central-1 Frankfurt)
    # Infra rÃ©duite : 2 instances (vs 6 en prod)
    aws autoscaling create-auto-scaling-group \
      --auto-scaling-group-name payment-api-dr \
      --min-size 2 \
      --max-size 40 \
      --region eu-central-1

    # RDS avec rÃ©plication cross-region
    aws rds create-db-instance-read-replica \
      --db-instance-identifier payment-db-dr \
      --source-db-instance-identifier payment-db \
      --region eu-central-1

    # Route 53 health check et failover
    aws route53 change-resource-record-sets \
      --hosted-zone-id Z123 \
      --change-batch file://failover-config.json
    ```

    **En cas de disaster :**
    1. Route 53 dÃ©tecte la panne (healthcheck KO)
    2. Bascule DNS automatique vers DR (2-3 min)
    3. ASG scale up en DR (5-10 min)
    4. RDS replica promoted en primary (5 min)
    â†’ **RTO total : 15-20 min** âœ… (objectif : 1h)

    **4. Configuration auto-scaling (pics x10)**

    ```bash
    # Policy: Scale up si CPU > 70%
    aws autoscaling put-scaling-policy \
      --auto-scaling-group-name payment-api-asg \
      --policy-name scale-up \
      --scaling-adjustment 3 \
      --adjustment-type ChangeInCapacity

    # CloudWatch alarm trigger
    aws cloudwatch put-metric-alarm \
      --alarm-name high-cpu \
      --metric-name CPUUtilization \
      --threshold 70 \
      --comparison-operator GreaterThanThreshold \
      --evaluation-periods 2 \
      --alarm-actions arn:aws:autoscaling:xxx:policy/scale-up

    # Policy: Scale down si CPU < 30%
    aws autoscaling put-scaling-policy \
      --policy-name scale-down \
      --scaling-adjustment -1 \
      --adjustment-type ChangeInCapacity \
      --cooldown 300
    ```

    **ParamÃ¨tres pour gÃ©rer x10 :**
    - Normal : 6 instances (10 000 tx/jour = ~417 tx/h/instance)
    - Pic : jusqu'Ã  40 instances (100 000 tx/jour)
    - Scaling progressif : +3 instances toutes les 2 min si besoin
    - Warmup period : 180s (le temps que l'app dÃ©marre)

---

## Navigation

| PrÃ©cÃ©dent | Suivant |
|-----------|---------|
| [â† Module 4 : SÃ©curitÃ© & ConformitÃ©](04-module.md) | [Module 6 : FinOps & CoÃ»ts â†’](06-module.md) |
